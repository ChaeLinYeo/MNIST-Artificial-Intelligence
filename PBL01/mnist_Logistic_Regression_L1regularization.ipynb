{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Automatically created module for IPython interactive environment\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/macbook/anaconda3/lib/python3.7/site-packages/sklearn/linear_model/sag.py:334: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  \"the coef_ did not converge\", ConvergenceWarning)\n",
      "/Users/macbook/anaconda3/lib/python3.7/site-packages/sklearn/linear_model/sag.py:334: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  \"the coef_ did not converge\", ConvergenceWarning)\n",
      "/Users/macbook/anaconda3/lib/python3.7/site-packages/sklearn/linear_model/sag.py:334: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  \"the coef_ did not converge\", ConvergenceWarning)\n",
      "/Users/macbook/anaconda3/lib/python3.7/site-packages/sklearn/linear_model/sag.py:334: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  \"the coef_ did not converge\", ConvergenceWarning)\n",
      "/Users/macbook/anaconda3/lib/python3.7/site-packages/sklearn/linear_model/sag.py:334: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  \"the coef_ did not converge\", ConvergenceWarning)\n",
      "/Users/macbook/anaconda3/lib/python3.7/site-packages/sklearn/linear_model/sag.py:334: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  \"the coef_ did not converge\", ConvergenceWarning)\n",
      "/Users/macbook/anaconda3/lib/python3.7/site-packages/sklearn/linear_model/sag.py:334: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  \"the coef_ did not converge\", ConvergenceWarning)\n",
      "/Users/macbook/anaconda3/lib/python3.7/site-packages/sklearn/linear_model/sag.py:334: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  \"the coef_ did not converge\", ConvergenceWarning)\n",
      "/Users/macbook/anaconda3/lib/python3.7/site-packages/sklearn/linear_model/sag.py:334: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  \"the coef_ did not converge\", ConvergenceWarning)\n",
      "/Users/macbook/anaconda3/lib/python3.7/site-packages/sklearn/linear_model/sag.py:334: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  \"the coef_ did not converge\", ConvergenceWarning)\n",
      "/Users/macbook/anaconda3/lib/python3.7/site-packages/sklearn/linear_model/sag.py:334: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  \"the coef_ did not converge\", ConvergenceWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "k fold: [0.92242857 0.91057143 0.92128571 0.92       0.91914286 0.92385714\n",
      " 0.925      0.91971429 0.91785714 0.92371429]\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "unsupported operand type(s) for /: 'LogisticRegression' and 'int'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-5-80f6d7fd0d19>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     70\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     71\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 72\u001b[0;31m \u001b[0mAVG\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmean\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mclf\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     73\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"avg : \"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mAVG\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     74\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/numpy/core/fromnumeric.py\u001b[0m in \u001b[0;36mmean\u001b[0;34m(a, axis, dtype, out, keepdims)\u001b[0m\n\u001b[1;32m   2918\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2919\u001b[0m     return _methods._mean(a, axis=axis, dtype=dtype,\n\u001b[0;32m-> 2920\u001b[0;31m                           out=out, **kwargs)\n\u001b[0m\u001b[1;32m   2921\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2922\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/numpy/core/_methods.py\u001b[0m in \u001b[0;36m_mean\u001b[0;34m(a, axis, dtype, out, keepdims)\u001b[0m\n\u001b[1;32m     85\u001b[0m             \u001b[0mret\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mret\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mret\u001b[0m \u001b[0;34m/\u001b[0m \u001b[0mrcount\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     86\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 87\u001b[0;31m         \u001b[0mret\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mret\u001b[0m \u001b[0;34m/\u001b[0m \u001b[0mrcount\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     88\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     89\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mret\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mTypeError\u001b[0m: unsupported operand type(s) for /: 'LogisticRegression' and 'int'"
     ]
    }
   ],
   "source": [
    "import time\n",
    "import numpy as np\n",
    "\n",
    "from sklearn.datasets import fetch_openml # MNIST data\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.utils import check_random_state\n",
    "from sklearn.model_selection import KFold, cross_val_score\n",
    "from sklearn.metrics import f1_score\n",
    "from sklearn.metrics import classification_report\n",
    "import matplotlib.pyplot as pyplot\n",
    "from matplotlib import pyplot as plt\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn import metrics\n",
    "import itertools\n",
    "\n",
    "print(__doc__)\n",
    "\n",
    "# Author: Arthur Mensch <arthur.mensch@m4x.org>\n",
    "# License: BSD 3 clause\n",
    "\n",
    "# Turn down for faster convergence\n",
    "t0 = time.time()\n",
    "train_samples = 60000\n",
    "\n",
    "# Load data from https://www.openml.org/d/554\n",
    "X, y = fetch_openml('mnist_784', version=1, return_X_y=True)\n",
    "\n",
    "random_state = check_random_state(0)\n",
    "permutation = random_state.permutation(X.shape[0])\n",
    "X = X[permutation]\n",
    "y = y[permutation]\n",
    "X = X.reshape((X.shape[0], -1))\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, train_size=train_samples, test_size=10000)\n",
    "\n",
    "scaler = StandardScaler()\n",
    "X_train = scaler.fit_transform(X_train)\n",
    "X_test = scaler.transform(X_test)\n",
    "\n",
    "# Turn up tolerance for faster convergence\n",
    "clf = LogisticRegression(C=10, penalty='l1', solver='saga', multi_class='multinomial')\n",
    "clf.fit(X_train, y_train)\n",
    "sparsity = np.mean(clf.coef_ == 0) * 100\n",
    "# score = clf.score(X_test, y_test)\n",
    "\n",
    "\n",
    "###k fold###\n",
    "kfold = KFold(n_splits=10)\n",
    "cvs = cross_val_score(clf, X, y, cv=kfold)\n",
    "print(\"k fold:\",cvs)\n",
    "\n",
    "\n",
    "# print(\"Sparsity with L1 penalty: %.2f%%\" % sparsity)\n",
    "# print(\"Test score with L1 penalty: %.4f\" % score)\n",
    "\n",
    "# coef = clf.coef_.copy()\n",
    "# plt.figure(figsize=(10, 5))\n",
    "# scale = np.abs(coef).max()\n",
    "# for i in range(10):\n",
    "#     l1_plot = plt.subplot(2, 5, i + 1)\n",
    "#     l1_plot.imshow(coef[i].reshape(28, 28), interpolation='nearest',\n",
    "#                    cmap=plt.cm.RdBu, vmin=-scale, vmax=scale)\n",
    "#     l1_plot.set_xticks(())\n",
    "#     l1_plot.set_yticks(())\n",
    "#     l1_plot.set_xlabel('Class %i' % i)\n",
    "# plt.suptitle('Classification vector for...')\n",
    "\n",
    "\n",
    "AVG = np.mean(clf)\n",
    "print(\"avg : \", AVG)\n",
    "\n",
    "run_time = time.time() - t0\n",
    "print('Example run in %.3f s' % run_time)\n",
    "plt.show()\n",
    "print(\"Sparsity with L1 penalty: %.2f%%\" % sparsity)\n",
    "\n",
    "def plot_confusion_matrix(cm, classes, \n",
    "                          normalize=False, \n",
    "                          title='Confusion matrix', \n",
    "                          cmap=pyplot.cm.Blues):\n",
    "#     ***\n",
    "#     This function prints and plots the confusion matrix.\n",
    "#     Normalizaton can be applied by setting 'normalize=True'.\n",
    "#     ***\n",
    "    \n",
    "    if normalize:\n",
    "        cm = cm.astype('float') / cm.sum(axis=1)[:,np.newaxis]\n",
    "        print(\"Normalized confusion matrix\")\n",
    "    else:\n",
    "        print('Confusion matrix, without normalization')\n",
    "    print(cm)\n",
    "    \n",
    "    pyplot.imshow(cm, interpolation = 'nearest', cmap = cmap)\n",
    "    pyplot.title(title)\n",
    "    pyplot.colorbar()\n",
    "    tick_marks = np.arange(len(classes))\n",
    "    pyplot.xticks(tick_marks, classes, rotation=45)\n",
    "    pyplot.yticks(tick_marks, classes)\n",
    "    \n",
    "    fmt = '.2f' if normalize else 'd'\n",
    "    thresh = cm.max() / 2.\n",
    "    for i,j in itertools.product(range(cm.shape[0]), range(cm.shape[1])):\n",
    "        pyplot.text(j, i, format(cm[i,j], fmt),\n",
    "                 horizontalalignment = \"center\",\n",
    "                 color=\"white\" if cm[i,j] > thresh else \"black\")\n",
    "        \n",
    "    pyplot.tight_layout()\n",
    "    pyplot.ylabel('True label')\n",
    "    pyplot.xlabel('Predicted label')\n",
    "    \n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn import metrics\n",
    "import itertools\n",
    "\n",
    "# svc = svc.fit(X,Y)\n",
    "Y_pred = clf.predict(X_test)\n",
    "cm = metrics.confusion_matrix(y_test,Y_pred)\n",
    "plot_confusion_matrix(cm, [\"0\",\"1\", \"2\", \"3\", \"4\", \"5\", \"6\", \"7\", \"8\", \"9\"], \n",
    "                      normalize=False)\n",
    "y_pred = clf.predict(X_test)\n",
    "f1 = f1_score(y_test, y_pred, average =None)\n",
    "print(format(f1))\n",
    "\n",
    "print(classification_report(y_test, Y_pred))\n",
    "\n",
    "metrics.f1_score(y_test,Y_pred,average='micro')\n",
    "\n",
    "print(__doc__)\n",
    "\n",
    "# Author: Arthur Mensch <arthur.mensch@m4x.org>\n",
    "# License: BSD 3 clause\n",
    "\n",
    "# Turn down for faster convergence\n",
    "t0 = time.time()\n",
    "train_samples = 60000\n",
    "\n",
    "# Load data from https://www.openml.org/d/554\n",
    "X, y = fetch_openml('mnist_784', version=1, return_X_y=True)\n",
    "\n",
    "random_state = check_random_state(0)\n",
    "permutation = random_state.permutation(X.shape[0])\n",
    "X = X[permutation]\n",
    "y = y[permutation]\n",
    "X = X.reshape((X.shape[0], -1))\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, train_size=train_samples, test_size=10000)\n",
    "\n",
    "scaler = StandardScaler()\n",
    "X_train = scaler.fit_transform(X_train)\n",
    "X_test = scaler.transform(X_test)\n",
    "\n",
    "# Turn up tolerance for faster convergence\n",
    "clf = LogisticRegression(C=10, penalty='l1', solver='saga', multi_class='multinomial')\n",
    "clf.fit(X_train, y_train)\n",
    "sparsity = np.mean(clf.coef_ == 0) * 100\n",
    "# score = clf.score(X_test, y_test)\n",
    "\n",
    "\n",
    "###k fold###\n",
    "kfold = KFold(n_splits=10)\n",
    "clf = cross_val_score(clf, X, y, cv=kfold)\n",
    "print(\"k fold:\",clf)\n",
    "\n",
    "\n",
    "# print(\"Sparsity with L1 penalty: %.2f%%\" % sparsity)\n",
    "# print(\"Test score with L1 penalty: %.4f\" % score)\n",
    "\n",
    "# coef = clf.coef_.copy()\n",
    "# plt.figure(figsize=(10, 5))\n",
    "# scale = np.abs(coef).max()\n",
    "# for i in range(10):\n",
    "#     l1_plot = plt.subplot(2, 5, i + 1)\n",
    "#     l1_plot.imshow(coef[i].reshape(28, 28), interpolation='nearest',\n",
    "#                    cmap=plt.cm.RdBu, vmin=-scale, vmax=scale)\n",
    "#     l1_plot.set_xticks(())\n",
    "#     l1_plot.set_yticks(())\n",
    "#     l1_plot.set_xlabel('Class %i' % i)\n",
    "# plt.suptitle('Classification vector for...')\n",
    "\n",
    "\n",
    "AVG = np.mean(cvs)\n",
    "print(\"avg : \", AVG)\n",
    "\n",
    "run_time = time.time() - t0\n",
    "print('Example run in %.3f s' % run_time)\n",
    "plt.show()\n",
    "print(\"Sparsity with L1 penalty: %.2f%%\" % sparsity)\n",
    "\n",
    "def plot_confusion_matrix(cm, classes, \n",
    "                          normalize=False, \n",
    "                          title='Confusion matrix', \n",
    "                          cmap=pyplot.cm.Blues):\n",
    "#     ***\n",
    "#     This function prints and plots the confusion matrix.\n",
    "#     Normalizaton can be applied by setting 'normalize=True'.\n",
    "#     ***\n",
    "    \n",
    "    if normalize:\n",
    "        cm = cm.astype('float') / cm.sum(axis=1)[:,np.newaxis]\n",
    "        print(\"Normalized confusion matrix\")\n",
    "    else:\n",
    "        print('Confusion matrix, without normalization')\n",
    "    print(cm)\n",
    "    \n",
    "    pyplot.imshow(cm, interpolation = 'nearest', cmap = cmap)\n",
    "    pyplot.title(title)\n",
    "    pyplot.colorbar()\n",
    "    tick_marks = np.arange(len(classes))\n",
    "    pyplot.xticks(tick_marks, classes, rotation=45)\n",
    "    pyplot.yticks(tick_marks, classes)\n",
    "    \n",
    "    fmt = '.2f' if normalize else 'd'\n",
    "    thresh = cm.max() / 2.\n",
    "    for i,j in itertools.product(range(cm.shape[0]), range(cm.shape[1])):\n",
    "        pyplot.text(j, i, format(cm[i,j], fmt),\n",
    "                 horizontalalignment = \"center\",\n",
    "                 color=\"white\" if cm[i,j] > thresh else \"black\")\n",
    "        \n",
    "    pyplot.tight_layout()\n",
    "    pyplot.ylabel('True label')\n",
    "    pyplot.xlabel('Predicted label')\n",
    "    \n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn import metrics\n",
    "import itertools\n",
    "\n",
    "# svc = svc.fit(X,Y)\n",
    "Y_pred = clf.predict(X_test)\n",
    "cm = metrics.confusion_matrix(y_test,Y_pred)\n",
    "plot_confusion_matrix(cm, [\"0\",\"1\", \"2\", \"3\", \"4\", \"5\", \"6\", \"7\", \"8\", \"9\"], \n",
    "                      normalize=False)\n",
    "y_pred = clf.predict(X_test)\n",
    "f1 = f1_score(y_test, y_pred, average =None)\n",
    "print(format(f1))\n",
    "\n",
    "print(classification_report(y_test, Y_pred))\n",
    "\n",
    "metrics.f1_score(y_test,Y_pred,average='micro')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
